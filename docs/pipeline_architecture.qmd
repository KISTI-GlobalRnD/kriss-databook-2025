# 1.2 파이프라인 아키텍처 / Pipeline Architecture

The keyword extraction pipeline contains four mandatory stages and one optional canonicalisation pass.
Each stage corresponds to a method on `KeywordExtractionPipeline`.

## 세부 목차 / Section Outline

1. Stage 0 – Vectoriser Fit
2. Stage 1 – Cluster Aggregation
3. Stage 2 – Scoring & Candidate Selection
4. Stage 2.5 – Canonicalisation (Optional)
5. Stage 2.6 – Global Stopword Pruning
6. Stage 2.7 – Secondary LLM Canonicalisation
7. Stage 2.8 – Priority Integration & Snapshots
8. Stage 3 – Year Series
9. Export & Diagnostics

## Stage 0 – Vectoriser Fit / 벡터라이저 학습

- Stream titles and abstracts, optionally weighting titles higher.
- Fit separate `CountVectorizer` instances for unigrams and phrases.
- Persist vectorisers with `joblib.dump` so that subsequent runs can skip refitting.
- Append auxiliary surface forms (e.g., WoS author keywords) to the streamed text at this point so that their counts flow through every downstream stage. / 부가 메타데이터(예: 저자 키워드)는 이 시점에 텍스트에 합쳐 Stage 1 이후까지 전파되도록 합니다.
- Disable the built-in stopword list by setting `use_default_stopwords=False` when deferring stopword
  cleansing to Stage 2.5. / 불용어 정리를 Stage 2.5 이후로 미루려면 `use_default_stopwords=False` 로 설정합니다.
- HTML-style tags such as `<sup>` are stripped during normalisation to avoid artefact tokens (`sup sup`). /
  정규화 단계에서 `<sup>` 같은 태그를 제거하여 `sup sup` 과 같은 잡음 토큰을 방지합니다.

## Stage 1 – Cluster Aggregation / 클러스터 집계

- Transform incoming batches into sparse document-term matrices.
- Map document IDs to cluster indices and sum counts per cluster.
- Track both total term frequency and document coverage for later filtering.

### Pseudocode (EN)

```
for batch in stream_documents():
    X_uni = vec_uni.transform(batch.texts)
    codes = map_to_cluster_indices(batch.cluster_ids)
    C_uni += group_sum(X_uni, codes)
    DF_uni += group_sum(X_uni > 0, codes)
    if vec_phrase:
        X_phrase = vec_phrase.transform(batch.texts)
        C_phrase += group_sum(X_phrase, codes)
        DF_phrase += group_sum(X_phrase > 0, codes)
```

### 단계 요약 (KO)

```
for batch in 문서 스트림:
    X_uni = vec_uni.transform(batch.texts)
    codes = 클러스터 인덱스 목록
    C_uni에 group_sum(X_uni, codes) 누적
    DF_uni에 group_sum(X_uni > 0, codes) 누적
    바이그램/트라이그램도 동일하게 처리
```

## Stage 2 – Scoring & Candidate Selection / 스코어링 및 후보 선택

- Compute class-based TF-IDF (c‑TF‑IDF) matrices.
- Optionally compute Log-Likelihood Ratio (LLR) per term using cluster vs. remainder counts.
- Apply coverage thresholds, remove subphrases, and run Maximal Marginal Relevance (MMR) with Jaccard
  similarity to reduce redundancy.
- The resulting candidate list is already a filtered subset of the vectoriser vocabulary; only terms that pass coverage and redundancy checks progress to Stage 2.5. / 이 단계 결과는 벡터라이저 전체 어휘 중 필터를 통과한 용어만 남긴 부분집합입니다.
- Pure stopword n-grams (예: “in the”, “of the”) are discarded automatically at this step. / 순수 불용어 n-그램은 이 단계에서 자동으로 제거됩니다.
- Each row now tracks `frequency`, `doc_coverage`, and (post-canonicalisation) `source_terms` metadata. / 행마다 빈도, 커버리지, 그리고 정규화 이후에는 `source_terms` 메타데이터를 보존합니다.

## Stage 2.5 – Canonicalisation (Optional) / 정규화(선택)

- Request LLM or domain dictionary mappings to merge plural/singular, spelling variants, or multilingual terms.
- Supported actions: `keep`, `merge_into`, `translate`, `drop`. Canonical terms retain a `source_terms` list so downstream metrics still aggregate over every alias. / LLM 응답은 `keep`, `merge_into`, `translate`, `drop` 중 하나를 선택하며, 최종 용어는 `source_terms` 목록으로 원본 표면형을 추적합니다.
- Combined scores and coverage are recomputed on the merged buckets while the underlying vectorisers remain unchanged. / 정규화 이후 병합된 버킷을 기준으로 점수와 커버리지를 재계산하며 벡터라이저는 그대로 유지됩니다.
- Alias responses are cached under `alias_cache_path` so reruns skip repeated LLM calls while Stage 2 terms are recomputed every time. / LLM 응답만 `alias_cache_path`에 캐시되어 재실행 시 호출을 생략하고 Stage 2 용어는 매번 새로 계산됩니다.
- The prompt enforces `drop` for pure stopword phrases so low-information terms do not persist. / 프롬프트는
  순수 불용어 구절에 대해 `drop`을 강제하여 저정보 용어가 남지 않도록 합니다.

## Stage 2.6 – Global Stopword Pruning / 전역 불용어 제거

- Cross-check canonical terms against global document coverage (from `Output/keywords_set.csv`) and
  drop any residual high-frequency stopwords that survived Stage 2.5. / Stage 2.5 이후에도 남은 전역 불용어는 `Output/keywords_set.csv`의 문서 커버리지 지표를 이용해 제거합니다.
- Persist the drop manifest to `artifacts/canonicalise/main-drop/mapping/` so QA reviewers can audit
  which aliases were filtered out. / 제거 내역은 `artifacts/canonicalise/main-drop/mapping/`에 저장하여 QA 검토가 가능하도록 합니다.

## Stage 2.7 – Secondary LLM Canonicalisation / 2차 LLM 정규화

- Run a focused GPT-5 pass (`artifacts/canonicalise/main-gpt/mapping/`) on the surviving vocabulary to
  merge edge-case variants without disturbing previously approved terms. / 잔여 변이만을 대상으로 GPT-5 정규화를 실행해 기존 승인 용어는 유지합니다.
- Reuse the same alias action schema, ensuring traceability across both canonicalisation rounds. / 동일한 alias 스키마를 사용하여 두 차수의 정규화 결과를 추적할 수 있도록 합니다.

## Stage 2.8 – Priority Integration & Snapshots / 우선순위 통합 및 스냅샷

- Consolidate tables in the order `main → main-drop → main-gpt`, producing the canonical export at
  `artifacts/second_canonical.csv`. / `main → main-drop → main-gpt` 순서로 통합하여 `artifacts/second_canonical.csv`에 저장합니다.
- Emit `Output/canonical.csv` as the analyst-ready deck delivered before manual cleansing begins. / 수동 정제 전에 분석자에게 제공하는 최종 키워드 세트는 `Output/canonical.csv`에 저장됩니다.

## Stage 3 – Year Series / 연도별 집계

- Reuse the restricted vocabulary of selected keywords.
- Replay the document stream to collect `{year: frequency}` counters for each canonical keyword, summing across all `source_terms`. / `source_terms`에 속한 모든 별칭을 합산해 `{year: frequency}` 시계열을 계산합니다.
- Capture per-cluster/year unigram totals (`pipeline.cluster_year_token_denoms`) so ppm/log-lift style normalisation is available downstream. If \(n_{c,y}(t)\) denotes the term count and \(N_{c,y}\) the unigram denominator, you can compute \(\mathrm{ppm}_{c,y}(t) = 10^{6} \cdot \tfrac{n_{c,y}(t)}{N_{c,y}}\) or other log-lift style metrics. / 클러스터·연도별 유니그램 총량을 `pipeline.cluster_year_token_denoms`에 보관하여, \(n_{c,y}(t)\) (용어 빈도)와 \(N_{c,y}\) (유니그램 분모)를 이용해 \(\mathrm{ppm}_{c,y}(t) = 10^{6} \cdot \tfrac{n_{c,y}(t)}{N_{c,y}}\) 같은 정규화 지표를 계산할 수 있습니다.
- Default normalised curves (`ppm_series`, `loglift_series`, `bayesian_log_odds_series`) are attached to
  the canonical table for immediate plotting. / 기본 정규화 시계열(`ppm_series`, `loglift_series`,
  `bayesian_log_odds_series`)이 canonical 테이블에 포함됩니다.
- Export enriched tables that include scores, raw counts, coverage, and temporal signals.
- Ensure this replay happens after any canonicalisation or manual vocabulary curation so that the time
  series reflect the cleaned term set. / 정규화나 수동 정제가 끝난 뒤에 재생을 수행하여 정제된 용어 기준의 시계열을 얻습니다.
- A final cleanup step removes any residual stopword-only n-grams before export. / 최종 단계에서 순수
  불용어 n-그램을 제거한 뒤 결과를 내보냅니다.

---

**Checklist / 점검표**

- [ ] Vectorisers persisted (`vec_uni`, `vec_phrase`).
- [ ] Cluster matrices stored (`C_*`, `DF_*`, `cluster_doc_counts`).
- [ ] Candidate tables saved (`top_df`) for restart.
- [ ] Global stopword drop manifest archived (`artifacts/canonicalise/main-drop/mapping/`).
- [ ] GPT-5 canonical mappings stored (`artifacts/canonicalise/main-gpt/mapping/`).
- [ ] Consolidated canonical exports written (`artifacts/second_canonical.csv`, `Output/canonical.csv`).
- [ ] Year series cached (`term_year`) for downstream trend analysis.
